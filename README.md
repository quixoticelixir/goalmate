# Goal Decomposer Prototype

Prototype full-stack app: user enters a goal on the frontend, backend returns a list of sub-goals (currently heuristic, later can be powered by a neural model).

## Stack

- Frontend: static HTML + vanilla JS (`public/index.html`, `public/main.js`)
- Backend: Node.js + Express (`src/server.js`, `src/services/goalDecomposer.js`)

## How to run locally

From the project root:

```bash
npm install
npm run dev
```

Then open:

- `http://localhost:3000` – frontend
- `POST http://localhost:3000/api/goals/decompose` – API (JSON body: `{ "goal": "your goal" }`)

## Where to plug a neural model

The function `decomposeGoal` in `src/services/goalDecomposer.js` is the place to integrate a real model:

- Call an external API (e.g. OpenAI, other provider)
- Call a local model server via HTTP
- Call a separate Python service

For now it uses a simple heuristic (`fakeHeuristicBreakdown`) so the prototype works end-to-end without external services.

## Using a real model (optional)

### Option 1: Hugging Face Inference API (recommended for experiments)

The project supports calling a text-generation model hosted on the Hugging Face Inference API.

1. Create an API token:
   - Go to your Hugging Face account.
   - Create an access token with permission to use the Inference API.
   - Copy it (keep it secret, do not commit it to Git).

2. Pick a model that supports text generation, for example:
   - `mistralai/Mistral-7B-Instruct-v0.2`
   - or another instruct / chat model you have access to.

3. Create or update the `.env` file in the project root (same folder as `package.json`):

   ```env
   HF_API_KEY=your_hf_token_here
   HF_MODEL=Qwen/Qwen2.5-VL-7B-Instruct
   USE_HF=true
   ```

   - `HF_API_KEY` – your Hugging Face access token.
   - `HF_MODEL` – the full model id on Hugging Face (e.g. `mistralai/Mistral-7B-Instruct-v0.2`).
   - `USE_HF` – if `true`, backend will try Hugging Face first and fall back to other options (OpenAI if configured, then heuristic).

4. Install dependencies and run:

   ```bash
   npm install
   npm run dev
   ```

5. Open the app at `http://localhost:3000`, enter a goal and click the button:
   - If Hugging Face is configured correctly, sub-goals are generated by the HF-hosted model.
   - If there is a problem (no token, invalid model, etc.), backend logs the error and falls back to other providers or the built-in heuristic.

### Option 2: OpenAI model

The project also contains an integration stub for OpenAI in `src/services/goalDecomposer.js`.

1. Create an API key:
   - Go to the OpenAI platform.
   - Create an API key in your account.
   - Copy it (keep it secret, do not commit it to Git).

2. Create or update `.env` in the project root:

   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   OPENAI_MODEL=gpt-4.1-mini
   USE_OPENAI=true
   ```

   - `OPENAI_API_KEY` – your secret key from OpenAI.
   - `OPENAI_MODEL` – any chat model your account has access to (e.g. `gpt-4.1-mini`).
   - `USE_OPENAI` – if `true`, backend will try OpenAI (after Hugging Face, if configured) and fall back to the heuristic if there is an error.

3. Install dependencies and run:

   ```bash
   npm install
   npm run dev
   ```

4. Open the app at `http://localhost:3000`, enter a goal and click the button:
   - If one of the remote providers is configured correctly, sub-goals are generated by that model.
   - If there is a problem with all remote providers, backend logs the errors and automatically falls back to the built-in heuristic.
